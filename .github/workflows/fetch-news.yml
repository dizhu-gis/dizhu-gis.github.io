name: Fetch Latest News from GeoDI RSS

on:
  workflow_dispatch: # Only manual trigger - no automatic schedule

permissions:
  contents: write # Allow writing to repository

jobs:
  fetch-news:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Fetch RSS and convert to JSON
      run: |
        python -c "
        import urllib.request
        import json
        import xml.etree.ElementTree as ET
        from datetime import datetime
        import os
        
        # Create data directory if it doesn't exist
        os.makedirs('data', exist_ok=True)
        
        # Fetch RSS feed
        rss_url = 'https://geodi.umn.edu/rss.xml'
        print(f'Fetching RSS from: {rss_url}')
        
        try:
            # Create request with proper headers to avoid 403 error
            req = urllib.request.Request(rss_url)
            req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')
            req.add_header('Accept-Language', 'en-US,en;q=0.5')
            req.add_header('Accept-Encoding', 'gzip, deflate')
            req.add_header('Connection', 'keep-alive')
            
            # Use urllib with headers
            with urllib.request.urlopen(req, timeout=30) as response:
                rss_content = response.read()
            print(f'RSS fetch successful, status: {response.status}')
            print(f'Content length: {len(rss_content)} bytes')
            print(f'Content type: {response.headers.get(\"Content-Type\", \"unknown\")}')
            
            # Debug: show first 500 characters
            content_preview = rss_content.decode('utf-8', errors='ignore')[:500]
            print(f'Content preview: {content_preview}')
            
        except Exception as e:
            print(f'Error fetching RSS: {e}')
            raise
        
        # Parse XML
        try:
            # Try to decode content properly
            if isinstance(rss_content, bytes):
                rss_text = rss_content.decode('utf-8', errors='ignore')
            else:
                rss_text = str(rss_content)
            
            # Remove any BOM or leading whitespace
            rss_text = rss_text.strip()
            
            # Check if it looks like XML
            if not rss_text.startswith('<?xml') and not rss_text.startswith('<rss'):
                print('Warning: Content does not appear to be XML')
                print(f'First 100 chars: {rss_text[:100]}')
            
            root = ET.fromstring(rss_text)
            print('XML parsing successful')
        except Exception as e:
            print(f'Error parsing XML: {e}')
            print(f'First 200 chars of content: {rss_text[:200] if \"rss_text\" in locals() else \"N/A\"}')
            raise
        
        # Extract items (RSS format)
        items = []
        for item in root.findall('.//item'):
            title_elem = item.find('title')
            link_elem = item.find('link')
            description_elem = item.find('description')
            pubDate_elem = item.find('pubDate')
            
            if title_elem is not None and link_elem is not None:
                title = title_elem.text.strip() if title_elem.text else ''
                link = link_elem.text.strip() if link_elem.text else ''
                description = description_elem.text.strip() if description_elem is not None and description_elem.text else ''
                pubDate = pubDate_elem.text.strip() if pubDate_elem is not None and pubDate_elem.text else ''
                
                # Clean description (remove HTML tags)
                import re
                description = re.sub(r'<[^>]+>', '', description)
                description = description.replace('&nbsp;', ' ').strip()
                
                # Limit description length
                if len(description) > 200:
                    description = description[:200] + '...'
                
                items.append({
                    'title': title,
                    'description': description,
                    'url': link,
                    'date': pubDate
                })
        
        print(f'Found {len(items)} total items in RSS feed')
        
        # Get top 3 items
        top_items = items[:3]
        
        # Create output data
        output_data = {
            'last_updated': datetime.now().isoformat(),
            'source': rss_url,
            'items': top_items
        }
        
        # Save to JSON file
        try:
            with open('data/latest-news.json', 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            print(f'Successfully saved {len(top_items)} news items to data/latest-news.json')
        except Exception as e:
            print(f'Error saving JSON file: {e}')
            raise
        
        for i, item in enumerate(top_items, 1):
            print(f'{i}. {item[\"title\"]}')
        "
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/latest-news.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Update latest news from GeoDI RSS feed"
        git push 