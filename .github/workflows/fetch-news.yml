name: Fetch Latest News from GeoDI RSS

on:
  workflow_dispatch: # Only manual trigger - no automatic schedule

permissions:
  contents: write # Allow writing to repository

jobs:
  fetch-news:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Fetch RSS and convert to JSON
      run: |
        python -c "
        import urllib.request
        import json
        import xml.etree.ElementTree as ET
        from datetime import datetime
        import os
        import gzip
        import re
        import time
        import random
        
        # Create data directory if it doesn't exist
        os.makedirs('data', exist_ok=True)
        
        # Fetch RSS feed
        rss_url = 'https://geodi.umn.edu/rss.xml'
        print(f'Fetching RSS from: {rss_url}')
        
        try:
            # Create request with proper headers to avoid 403 error
            req = urllib.request.Request(rss_url)
            req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')
            req.add_header('Accept-Language', 'en-US,en;q=0.5')
            req.add_header('Connection', 'keep-alive')
            
            # Use urllib with headers
            with urllib.request.urlopen(req, timeout=30) as response:
                rss_content = response.read()
            print(f'RSS fetch successful, status: {response.status}')
            print(f'Content length: {len(rss_content)} bytes')
            
            # Handle gzip compression if present
            if response.headers.get('Content-Encoding') == 'gzip':
                print('Decompressing gzipped content...')
                rss_content = gzip.decompress(rss_content)
                print(f'Decompressed content length: {len(rss_content)} bytes')
            
        except Exception as e:
            print(f'Error fetching RSS: {e}')
            raise
        
        # Parse XML
        try:
            if isinstance(rss_content, bytes):
                rss_text = rss_content.decode('utf-8', errors='ignore')
            else:
                rss_text = str(rss_content)
            
            rss_text = rss_text.strip()
            root = ET.fromstring(rss_text)
            print('XML parsing successful')
        except Exception as e:
            print(f'Error parsing XML: {e}')
            raise
        
        # Extract items (RSS format)
        items = []
        print('Starting to extract items from RSS feed...')
        
        for item in root.findall('.//item'):
            title_elem = item.find('title')
            link_elem = item.find('link')
            description_elem = item.find('description')
            pubDate_elem = item.find('pubDate')
            
            if title_elem is not None and link_elem is not None:
                title = title_elem.text.strip() if title_elem.text else ''
                link = link_elem.text.strip() if link_elem.text else ''
                description = description_elem.text.strip() if description_elem is not None and description_elem.text else ''
                pubDate = pubDate_elem.text.strip() if pubDate_elem is not None and pubDate_elem.text else ''
                
                print(f'Processing item: {title}')
                
                # Clean description (remove HTML tags)
                description = re.sub(r'<[^>]+>', '', description)
                description = description.replace('&nbsp;', ' ').strip()
                
                # Extract the first paragraph of actual content
                content_match = re.search(r'\s{12,}([^.!?]+[.!?])', description)
                
                if content_match:
                    extracted_content = content_match.group(1).strip()
                    print(f'Extracted content: {extracted_content[:100]}...')
                else:
                    # Fallback: try to find content after metadata patterns
                    lines = description.split('\n')
                    start_index = 0
                    
                    for i, line in enumerate(lines):
                        line = line.strip()
                        if (len(line) > 20 and 
                            ('research' in line.lower() or 
                             'article' in line.lower() or 
                             'paper' in line.lower() or
                             'study' in line.lower() or
                             'published' in line.lower())):
                            start_index = i
                            break
                    
                    content_lines = lines[start_index:]
                    extracted_content = ' '.join(content_lines).strip()
                    
                    sentence_match = re.search(r'^([^.!?]+[.!?])', extracted_content)
                    if sentence_match:
                        extracted_content = sentence_match.group(1).strip()
                    else:
                        extracted_content = extracted_content[:200].strip()
                    
                    print(f'Extracted content (fallback): {extracted_content[:100]}...')
                
                # Limit description length
                if len(extracted_content) > 200:
                    extracted_content = extracted_content[:200] + '...'
                
                items.append({
                    'title': title,
                    'description': extracted_content,
                    'url': link,
                    'date': pubDate
                })
        
        print(f'Found {len(items)} total items in RSS feed')
        
        # Get top 3 items
        top_items = items[:3]
        
        # Create output data with unique identifiers
        time.sleep(0.1)
        random_suffix = random.randint(1000, 9999)
        current_time = datetime.now().isoformat()
        
        output_data = {
            'last_updated': current_time,
            'source': rss_url,
            'items': top_items,
            'action_run_id': f'run_{int(time.time())}_{random_suffix}'
        }
        
        print(f'Generated JSON with timestamp: {output_data["last_updated"]}')
        print(f'Action run ID: {output_data["action_run_id"]}')
        print(f'Total items processed: {len(items)}')
        print(f'Top items selected: {len(top_items)}')
        
        # Save to JSON file
        try:
            with open('data/latest-news.json', 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            print(f'Successfully saved {len(top_items)} news items to data/latest-news.json')
        except Exception as e:
            print(f'Error saving JSON file: {e}')
            raise
        
        for i, item in enumerate(top_items, 1):
            print(f'{i}. {item[\"title\"]}')
        "
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Check if the JSON file exists
        if [ -f "data/latest-news.json" ]; then
            echo "‚úÖ JSON file exists, adding to git..."
            git add data/latest-news.json
            git status
            
            # Always commit and push
            echo "üîÑ Updating JSON file with fresh data..."
            git commit -m "Update latest news from GeoDI RSS feed - $(date)"
            git push
            echo "‚úÖ Successfully pushed fresh news data"
        else
            echo "‚ùå JSON file not found - Python script may have failed"
            exit 1
        fi 